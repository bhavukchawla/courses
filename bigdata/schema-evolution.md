
#### Prerequisites
 * Install Sqoop [Sqoop Installation on Dataproc cluster](../installation/sqoop-installation.md)
 * Setup retail_db database [Big Data Batch Pipeline](../bigdata/batch_pipeline.md)

#### Sqoop Import customers table from MySql to HDFS in AVRO format
```bash
./bin/sqoop import -Dmapreduce.job.classloader=true --connect jdbc:mysql://localhost/retail_db \
--username root --password password --table customers \
--as-avrodatafile \
--target-dir data/customers 
```

#### Create a Schema directory in HDFS
```bash
hdfs dfs -mkdir schema
```

#### Put customers.avsc file into HDFS 
**Note:** You will find ``customers.avsc`` file in your local **/usr/lib/sqoop/**
```bash
hdfs dfs -put /usr/lib/sqoop/customers.avsc schema/
```

#### Download avro tools jar
```bash
wget https://repo1.maven.org/maven2/org/apache/avro/avro-tools/1.8.2/avro-tools-1.8.2.jar
```

#### Open Hive shell
```bash
hive
```

#### {username} should be replaced below then Create table customerv1 as avro
```bash
CREATE EXTERNAL TABLE customerv1
STORED AS AVRO
LOCATION '/user/{username}/data/customers/'
TBLPROPERTIES ('avro.schema.url'='schema/customer.avsc');
```

#### Use the avro-tools-*.jar, to read schema of the file generated by sqoop. by executing following command
```bash
hdfs dfs -get data/customers .
java -jar ~/avro-tools-1.8.2.jar getschema customers/part-m-00000.avro
```

#### Edit the schema
```bash
nano customer.avsc
```
Add **pref_lang** field inside customer.avsc
```bash
  {
    "name" : "pref_lang",
    "type" : [ "null", "string" ],
    "default" : null,
    "columnName" : "pref_lang",
    "sqlType" : "12"
  } 
```
#### Rename **customer.avsc** to **customerv2.avsc** and put **customerv2.avsc** file in HDFS
Upload the avro schema file that you created in the last step back to HDFS
```bash  
mv customer.avsc customerv2.avsc 
hdfs dfs -put customerv2.avsc schema/
```

#### Create table customerv1 as avro
```bash
CREATE EXTERNAL TABLE customerv2
STORED AS AVRO
LOCATION '/user/{username}/data/customers/'
TBLPROPERTIES ('avro.schema.url'='schema/customerv2.avsc');
```

#### Insert new content in customerv2 table
```bash
INSERT INTO table customerv2 (customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode, pref_lang) VALUES (12436, "David", "Norwood", "xxxxxxxx", "xxxxxxxx", "3151 Sleepy Quail Promenade", "Passaic", "NJ", "07055", "ES");
```

#### Use hadoop jar avro-tools-*.jar <command> <args> to read schema from HDFS
```bash
hadoop jar ~/avro-tools-1.8.2.jar getschema data/customer/000000_0
```
